{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Assignment Number\n",
    "\n",
    "To load the correct settings, the number for the current assignment is set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSIGNMENT_NR = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages and Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "from canvasapi import Canvas\n",
    "from canvasapi.requester import Requester\n",
    "from canvas_connector.utils.canvas_utils import download_assignment_submissions\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from markdown_pdf import MarkdownPdf, Section\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import zipfile\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# Local imports\n",
    "from scripts.canvas_utils import update_canvas_grade, post_canvas_comments\n",
    "from scripts.jsonify import jsonify, jsonify_resources, analyze_jsonify_results\n",
    "from scripts.utils import ensure_folder_exists, create_file_list, parsed_submissions_quality_check, deduplicate_files_with_manual_fixes, load_latest_jsonified_student_submission, load_jsonified_resources\n",
    "from scripts.llm_utils import create_openai_message, prompt_gpt, format_with_default, format_and_compile_openai_messages\n",
    "from scripts.utils import extract_html_content, get_sum_points_for_pattern, get_weighted_points, deduplicate_highest_attempt\n",
    "from scripts.llm_report_utils import start_report_with_header, add_messages_to_report, add_text_to_report, add_prompt_and_response_to_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load global settings\n",
    "from scripts.settings import *\n",
    "\n",
    "# Load assignment specific settings\n",
    "ASSIGNMENT = ASSIGNMENTS[ASSIGNMENT_NR]\n",
    "ASSIGNMENT_ID = ASSIGNMENT[\"canvas\"][\"assignment_id\"]\n",
    "QUIZ_ID = ASSIGNMENT[\"canvas\"][\"quiz_id\"]\n",
    "R_QUIZ_QUESTION_ID = ASSIGNMENT[\"canvas\"][\"r_quiz_question_id\"]\n",
    "ADV_QUIZ_QUESTION_ID = ASSIGNMENT[\"canvas\"][\"adv_quiz_question_id\"]\n",
    "LOCK_GRADES_DATE = ASSIGNMENT[\"lock_grades_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Canvas API\n",
    "canvas_client = Canvas(os.getenv(\"CANVAS_API_URL\"), os.getenv(\"CANVAS_API_KEY\"))\n",
    "canvas_requester = Requester(os.getenv(\"CANVAS_API_URL\"), os.getenv(\"CANVAS_API_KEY\"))\n",
    "\n",
    "# Initialize OpenAI API\n",
    "if USE_UVA_OPENAI:\n",
    "    openai_client = OpenAI(api_key=os.getenv(\"UVA_OPENAI_API_KEY\"), \n",
    "                           base_url=os.getenv(\"UVA_OPENAI_BASE_URL\"))\n",
    "    if MODEL == \"gpt-4o\":\n",
    "        MODEL = \"gpt4o\" # OpenAI API uses a different model name\n",
    "else:\n",
    "    openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jsonify Resources\n",
    "\n",
    "To ensure the latest changes to rubrics, assignment, example solutions, or goals are captured, the resources are jsonified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions: 48 (R: 30, Radv: 8, Python: 10)\n",
      "rubrics: 48 (R: 30, Radv: 8, Python: 10)\n",
      "solutions: 48 (R: 30, Radv: 8, Python: 10)\n",
      "goals: 48 (R: 30, Radv: 8, Python: 10)\n",
      "weights: 48 (R: 30, Radv: 8, Python: 10)\n"
     ]
    }
   ],
   "source": [
    "analyze_jsonify_results(jsonify_resources(ASSIGNMENT_NR, RESOURCES_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Prepare Submissions\n",
    "\n",
    "All assignment submissions are downloaded and jsonified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download assignment submissions\n",
    "user_whitelist = [513294]\n",
    "user_blacklist = []\n",
    "out_paths = download_assignment_submissions(canvas_requester, COURSE_ID, ASSIGNMENT_ID, user_whitelist, user_blacklist, SUBMISSIONS_PATH + \"/user-{user_id}/assignment-{assignment_id}/user-{user_id}_ass-{assignment_id}_try-{attempt}_que-{question_id}_att-{attachment_id}\")\n",
    "\n",
    "# Jsonify submissions\n",
    "for out_path in out_paths:\n",
    "    jsonify(out_path, \".\".join(out_path.split(\".\")[0:-1]) + \".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some submissions may be formated incorrectly, despite instructing students how to format them and to validate them here before submitting: https://lukekorthals.shinyapps.io/pips-submission-validator/ \n",
    "\n",
    "Therefore, perform a quality check to make sure submissions were correctly parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found\n",
      "- 1 complete submissions\n",
      "- 0 incomplete submissions\n",
      "- 0 submissions with additional indicators\n"
     ]
    }
   ],
   "source": [
    "quality_check_df = parsed_submissions_quality_check(ASSIGNMENT_NR, ASSIGNMENT_ID)\n",
    "\n",
    "print(f\"Found\")\n",
    "print(f\"- {len(quality_check_df[quality_check_df[\"all_indicators_found\"]])} complete submissions\")\n",
    "print(f\"- {len(quality_check_df[~quality_check_df[\"all_indicators_found\"]])} incomplete submissions\")\n",
    "print(f\"- {len(quality_check_df[quality_check_df[\"contains_additional_indicators\"]])} submissions with additional indicators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the raw submissions by students with missing indicators to check if they are really missing or just not recognized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students with missing indicators \n",
    "quality_check_df[~quality_check_df[\"all_indicators_found\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the raw submissions by students with additional indicators and see if you udnerstand what went wrong and if you can fix it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students with missing indicators \n",
    "quality_check_df[quality_check_df[\"contains_additional_indicators\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to fix anything (e.g., because a student wrote #R 1 instead of #R1), copy the raw submission and append `_ManualFixes` before the file extension. Then rejsonify the manual fixes. The remainder of the pipeline will prefer files with ManualFixes over raw files. \n",
    "\n",
    "After jsonifying any files with ManualFixes, recheck the `quality_check_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jsonify submissions with manual fixes\n",
    "files_with_fixes = create_file_list(SUBMISSIONS_PATH, [\"_ManualFixes\"],[\".json\"])\n",
    "for file in files_with_fixes:\n",
    "    jsonify(file, \".\".join(file.split(\".\")[0:-1]) + \".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt LLM for Grading and Feedback\n",
    "The assignments of all students are graded and feedbacked by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#R1\n",
      "#R2\n",
      "#R3\n",
      "#R4\n",
      "#R5\n",
      "#R6\n",
      "#R7\n",
      "#R8\n",
      "#R9\n",
      "#R10\n",
      "#R11\n",
      "#R12\n",
      "#R13\n",
      "#R14\n",
      "#R15\n",
      "#R16\n",
      "#R17\n",
      "#R18\n",
      "#R19\n",
      "#R20\n",
      "#R21\n",
      "#R22\n",
      "#R23\n",
      "#R24\n",
      "#R25\n",
      "#R26\n",
      "#R27\n",
      "#R28\n",
      "#R29\n",
      "#R30\n",
      "#Radv1\n",
      "#Radv2\n",
      "#Radv3\n",
      "#Radv4\n",
      "#Radv5\n",
      "#Radv6\n",
      "#Radv7\n",
      "#Radv8\n",
      "#Python1\n",
      "#Python2\n",
      "#Python3\n",
      "#Python4\n",
      "#Python5\n",
      "#Python6\n",
      "#Python7\n",
      "#Python8\n",
      "#Python9\n",
      "#Python10\n"
     ]
    }
   ],
   "source": [
    "# Get user IDs\n",
    "user_ids = [user.split(\"-\")[1] for user in os.listdir(SUBMISSIONS_PATH) if user.startswith(\"user\")]\n",
    "\n",
    "# Get jsonified resources for this week\n",
    "resources = load_jsonified_resources(ASSIGNMENT_NR, \n",
    "                                     RESOURCES_PATH, \n",
    "                                     [\"questions\", \"solutions\", \"rubrics\", \"goals\", \"weights\"])\n",
    "\n",
    "# Load llm completion report templates\n",
    "header_template = open(\"resources/llm_report/llm_report_header_template.txt\", \"r\").read()\n",
    "\n",
    "# Prepare unformatted messages\n",
    "unformatted_grading_messages = [(\"system\", PROMPTS[\"grading\"][\"system_prompt\"]), \n",
    "                                (\"user\", PROMPTS[\"grading\"][\"user_prompt\"])]\n",
    "unformatted_feedback_qw_messages = [(\"system\", PROMPTS[\"feedback_questionwise\"][\"system_prompt\"]), \n",
    "                                    (\"user\", PROMPTS[\"feedback_questionwise\"][\"user_prompt\"])]\n",
    "unformatted_feedback_sum_messages = [(\"system\", PROMPTS[\"feedback_summary\"][\"system_prompt\"]), \n",
    "                                     (\"user\", PROMPTS[\"feedback_summary\"][\"user_prompt\"])]\n",
    "\n",
    "# Loop over all users\n",
    "for user_id in user_ids:\n",
    "    # Get student submission\n",
    "    submission, attempt = load_latest_jsonified_student_submission(ASSIGNMENT_ID, user_id, SUBMISSIONS_PATH)\n",
    "\n",
    "    # Initilize dicts\n",
    "    grading_dict = {}\n",
    "    feedback_dict = {}\n",
    "\n",
    "    # Initialize report\n",
    "    llm_report_out_path = f\"{SUBMISSIONS_PATH}/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_LLMCompletionReport.md\"\n",
    "    add_text_to_report(llm_report_out_path,\n",
    "                       text=format_with_default(header_template,\n",
    "                                                {\"model\": MODEL,\n",
    "                                                 \"grading_temperature\": GRADING_TEMPERATURE,\n",
    "                                                 \"feedback_temperature\": FEEDBACK_TEMPERATURE,\n",
    "                                                 \"n_choices_grading\": N_CHOICES_GRADING,\n",
    "                                                 \"n_choices_feedback\": N_CHOICES_FEEDBACK,\n",
    "                                                 \"student_id\": user_id,\n",
    "                                                 \"assignment_id\": ASSIGNMENT_ID}),\n",
    "                        start_new=True)\n",
    "    # Loop over all questions\n",
    "    i = 0\n",
    "    for indicator in resources[\"questions\"]:\n",
    "        i += 1\n",
    "        if i > 200:\n",
    "            break\n",
    "        print(indicator)\n",
    "\n",
    "        # Extract relevant information\n",
    "        formatting_dict = {\n",
    "            \"task\": resources[\"questions\"][indicator],\n",
    "            \"solution\": resources[\"solutions\"][indicator],\n",
    "            \"rubric\": resources[\"rubrics\"][indicator],\n",
    "            \"answer\": \"\\n\".join(submission[indicator]),\n",
    "            \"goal\": resources[\"goals\"][indicator]\n",
    "        }\n",
    "        \n",
    "\n",
    "        # Prompt for grading\n",
    "        messages = format_and_compile_openai_messages(unformatted_grading_messages, formatting_dict)\n",
    "        pkl_out_path = f\"{SUBMISSIONS_PATH}/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/pickled_completions/user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_task-{indicator}_prompt-grading_completion.pkl\"\n",
    "        completion = prompt_gpt(openai_client,\n",
    "                                MODEL, \n",
    "                                messages, \n",
    "                                pkl_out_path=pkl_out_path, \n",
    "                                n=N_CHOICES_GRADING,\n",
    "                                temperature=GRADING_TEMPERATURE)\n",
    "        \n",
    "        # Add first choice to grading dict\n",
    "        grading_dict[indicator] = completion.choices[0].message.content # TODO which choice to extraxct?\n",
    "\n",
    "        # Add chat completions to report\n",
    "        add_prompt_and_response_to_report(llm_report_out_path,\n",
    "                                          indicator,\n",
    "                                          \"Grading\",\n",
    "                                          messages,\n",
    "                                          completion)\n",
    "        \n",
    "        # Save grading to file\n",
    "        dat = pd.DataFrame({\n",
    "            \"user_id\": [user_id],\n",
    "            \"assignment_id\": [ASSIGNMENT_ID],\n",
    "            \"attempt\": [attempt],\n",
    "            \"grader\": [MODEL],\n",
    "            \"question\": [indicator],\n",
    "            \"points\": [float(extract_html_content(completion.choices[0].message.content, \"points\"))],\n",
    "            \"explanation\": [extract_html_content(completion.choices[0].message.content, \"explanation\")]\n",
    "        })\n",
    "        file_name = f\"{SUBMISSIONS_PATH}/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/grading/grading_user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_grader-{MODEL}_que-{indicator}.csv\"\n",
    "        ensure_folder_exists(file_name)\n",
    "        dat.to_csv(file_name, index=False)\n",
    "        \n",
    "        # Prompt for feedback\n",
    "        messages = format_and_compile_openai_messages(unformatted_feedback_qw_messages, formatting_dict)\n",
    "        pkl_out_path = f\"{SUBMISSIONS_PATH}/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/pickled_completions/user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_task-{indicator}_prompt-feedback-questionwise_completion.pkl\"\n",
    "        completion = prompt_gpt(openai_client,\n",
    "                                MODEL, \n",
    "                                messages, \n",
    "                                pkl_out_path=pkl_out_path, \n",
    "                                n=N_CHOICES_FEEDBACK,\n",
    "                                temperature=FEEDBACK_TEMPERATURE)\n",
    "        \n",
    "        # Add first choice to feedback dict\n",
    "        feedback_dict[indicator] = completion.choices[0].message.content # TODO which choice to extraxct?\n",
    "\n",
    "        # Add chat completions to report\n",
    "        add_prompt_and_response_to_report(llm_report_out_path,\n",
    "                                          None,\n",
    "                                          \"Feedback\",  \n",
    "                                          messages,\n",
    "                                          completion)\n",
    "        \n",
    "    # Prompt for feedback summary\n",
    "    feedback = \"\\n\\n\\n\".join([f\"{key}\\n{extract_html_content(value, 'feedback')}\" for key, value in feedback_dict.items()])\n",
    "    messages = format_and_compile_openai_messages(unformatted_feedback_sum_messages, {\"feedback\": feedback})\n",
    "    pkl_out_path = f\"{SUBMISSIONS_PATH}/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/pickled_completions/user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_prompt-feedback-summary_completion.pkl\"\n",
    "    completion = prompt_gpt(openai_client,\n",
    "                            MODEL, \n",
    "                            messages, \n",
    "                            pkl_out_path=pkl_out_path, \n",
    "                            n=N_CHOICES_FEEDBACK,\n",
    "                            temperature=FEEDBACK_TEMPERATURE)\n",
    "\n",
    "    # Add chat completions to report\n",
    "    add_prompt_and_response_to_report(llm_report_out_path,\n",
    "                                        \"Feedback Summary\",\n",
    "                                        \"Feedback\",\n",
    "                                        messages,\n",
    "                                        completion)\n",
    "\n",
    "    # Get LLM grade\n",
    "    # This calculation is specific to the PIPS 2025 course\n",
    "    points = {key: float(extract_html_content(value, \"points\")) for key, value in grading_dict.items()}\n",
    "    points_w = get_weighted_points(points, resources[\"weights\"])\n",
    "    points_r = round(get_sum_points_for_pattern(points_w, r\"#R(\\d+)\") * MAX_GRADE, 2)\n",
    "    points_radv = round(get_sum_points_for_pattern(points_w, r\"#Radv(\\d+)\") * MAX_GRADE, 2)\n",
    "    points_py = round(get_sum_points_for_pattern(points_w, r\"#Python(\\d+)\") * MAX_GRADE, 2)\n",
    "    points_adv = points_radv if points_radv > 0 else points_py\n",
    "    used_adv = \"You were graded based on Radv.\" if points_radv > 0 else \"You were graded based on Python.\"\n",
    "    grade = round(points_r + points_adv, 2)\n",
    "\n",
    "    # Save grade\n",
    "    dat = pd.DataFrame({\"user\": [user_id],\n",
    "                        \"assignment\": [ASSIGNMENT_ID],\n",
    "                        \"attempt\": [attempt],\n",
    "                        \"grader\": [MODEL],\n",
    "                        **points,\n",
    "                        \"points_r\": [points_r], \n",
    "                        \"points_radv\": [points_radv], \n",
    "                        \"points_py\": [points_py], \n",
    "                        \"points_adv\": [points_adv], \n",
    "                        \"used_adv\": [used_adv],\n",
    "                        \"grade\": [grade]})\n",
    "    dat.to_csv(f\"{SUBMISSIONS_PATH}/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/grading/grading_user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_grader-{MODEL}_que-combined.csv\")\n",
    "\n",
    "\n",
    "    # Generate Feedback report\n",
    "    pdf = MarkdownPdf()\n",
    "\n",
    "    # Header, summary, preliminary grade, coding challenge\n",
    "    section = f\"# Feedback Assignment {ASSIGNMENT_ID}\\n\\n\"\n",
    "    section += \"## Summary\\n\"\n",
    "    section += extract_html_content(completion.choices[0].message.content, \"summary\")\n",
    "    section += \"\\n\\n\"\n",
    "    section += \"## Coding Challenge\\n\"\n",
    "    section += \"We invite you to work on the following personalized coding challenge and submit your result on Canvas. Dont worry about being perfect, this is ungraded and just for your practice.\\n\\n\"\n",
    "    section += extract_html_content(completion.choices[0].message.content, \"coding-challenge\")\n",
    "    pdf.add_section(Section(section))\n",
    "\n",
    "    # Questionwise feedback\n",
    "    section = \"\"\n",
    "    for key, value in feedback_dict.items():\n",
    "        section += f\"## {key}\\n\"        \n",
    "        section += extract_html_content(value, \"feedback\")\n",
    "        section += \"\\n\\n--\\n\\n\"\n",
    "\n",
    "    pdf.add_section(Section(section))\n",
    "\n",
    "    # Save pdf\n",
    "    student_feedback_report_out_path = f\"{SUBMISSIONS_PATH}/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_LLMFeedback.pdf\"\n",
    "    pdf.save(student_feedback_report_out_path)\n",
    "\n",
    "    # Generate Grading report\n",
    "    pdf = MarkdownPdf()\n",
    "\n",
    "    # Header, summary, preliminary grade, coding challenge\n",
    "    section = f\"# LLM Grading Assignment {ASSIGNMENT_ID}\\n\\n\"\n",
    "    section += f\"**Points R:** {points_r}\\n\"\n",
    "    section += f\"**Points Radv:** {points_radv}\\n\"\n",
    "    section += f\"**Points Python:** {points_py}\\n\"\n",
    "    section += f\"{used_adv}\\n\"\n",
    "    section += f\"**Preliminary grade:** {grade}/10\\n\\n\"\n",
    "\n",
    "    \n",
    "    # Questionwise grading\n",
    "    for key, value in grading_dict.items():\n",
    "        # section = \"\"\n",
    "        section += f\"## {key} ({extract_html_content(value, \"points\")}/1)\\n\"\n",
    "        section += extract_html_content(value, \"explanation\")\n",
    "        section += \"\\n\\n\"\n",
    "    pdf.add_section(Section(section))\n",
    "\n",
    "    # Save pdf\n",
    "    student_grading_report_out_path = f\"{SUBMISSIONS_PATH}/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_LLMGrading.pdf\"\n",
    "    pdf.save(student_grading_report_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#R1': '<my-thoughts>\\n- The code uses the getwd() function.\\n- The code matches the example solution.\\n- There are no additional issues.\\n</my-thoughts>\\n<explanation>No points are deducted as the student used the correct function getwd(). Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R2': '<my-thoughts>\\n- The code uses setwd, which matches the rubric.\\n- The path does not end with the two specified folders .../r_course/week_1.\\n</my-thoughts>\\n<explanation>Subtracting 0.25 points because the path does not end with the two specified folders .../r_course/week_1. Calculation: 1 - 0.25 = 0.75</explanation><points>0.75</points>',\n",
       " '#R3': '<my-thoughts>\\n- The code uses list.files() which matches the rubric.\\n- The code uses ?list.files which matches the rubric.\\n- The student included extra text that was not required.\\n</my-thoughts>\\n<explanation>No points are deducted as the student correctly used list.files and ?list.files. The extra text does not affect the functionality of the code. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R4': '<my-thoughts>\\n- The code produces the correct variables hours_sleep and reported_happiness.\\n- hours_sleep is assigned the value 5 and reported_happiness is assigned the value \"3\".\\n- The student attempted to add the two variables.\\n- The comment explains why the addition cannot be performed.\\n- The explanation in the comment is correct but uses informal language (\"cant\" instead of \"cannot\").\\n</my-thoughts>\\n<explanation>Subtracting 0.25 points because the comment uses informal language (\"cant\" instead of \"cannot\"). Calculation: 1 - 0.25 = 0.75</explanation><points>0.75</points>',\n",
       " '#R5': '<my-thoughts>\\n- The code is explained correctly.\\n- The number of variables is stated correctly.\\n</my-thoughts>\\n<explanation>No points are deducted as the student correctly explained the code and stated the correct number of variables. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R6': '<my-thoughts>\\n- The code produces the correct output.\\n- The variable name is correct.\\n- The student used the print() function correctly.\\n- All requested lines are present.\\n</my-thoughts>\\n<explanation>No points are deducted as there are no typos in variable names and all requested lines are present. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R7': '<my-thoughts>\\n- The code produces a vector with elements 2, FALSE, -1.243, \"test\" which matches the rubric.\\n- The student correctly mentions that the datatype is \"character\".\\n- The student uses `typeof(a)` to check the datatype, which is correct.\\n- The comment correctly explains that all elements are converted to character.\\n</my-thoughts>\\n<explanation>No points are deducted as the student correctly defined the vector and mentioned the datatype as \"character\". Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R8': '<my-thoughts>\\n- The student correctly used the `na.rm = TRUE` argument in the `mean` function.\\n- The first line of code was not changed.\\n- The computed mean will be correct with the provided code.\\n</my-thoughts>\\n<explanation>No points are deducted as the student followed the instructions correctly and the computed mean will be correct. Calculation: 1 - 0 - 0 = 1</explanation><points>1</points>',\n",
       " '#R9': '<my-thoughts>\\n- The code installs the fortunes package.\\n- The code loads the fortunes package using library(fortunes).\\n- The code uses the fortune() function with the number 10, which is a valid month.\\n</my-thoughts>\\n<explanation>No points are deducted as the student mentioned the fortunes package and used the fortune() function with a valid month number. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R10': '<my-thoughts>\\n- The student correctly identified the forbidden mathematical operation as division by zero.\\n- The student included a comment explicitly mentioning division by zero.\\n</my-thoughts>\\n<explanation>No points are deducted as the student explicitly mentioned division by zero. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R11': '<my-thoughts>\\n- The code produces a vector of length 100 using rnorm, which is acceptable.\\n- The vector is not sorted.\\n- The correct elements (1st, 20th, and 80th) are extracted using indexing.\\n</my-thoughts>\\n<explanation>Subtracting 0.25 points because the vector is not sorted. Calculation: 1 - 0.25 = 0.75</explanation><points>0.75</points>',\n",
       " '#R12': '<my-thoughts>\\n- The code produces the correct output.\\n- The student used the correct function and variable.\\n- The solution matches the example solution.\\n</my-thoughts>\\n<explanation>No points deducted as the solution is correct and follows the rubric. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R13': '<my-thoughts>\\n- The student correctly identifies the second line as the one that does not work like the others.\\n- The student provides an accurate explanation that the second line compares two variables instead of assigning one variable to another.\\n</my-thoughts>\\n<explanation>No points are deducted as the student correctly identified the correct line and provided a valid explanation. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R14': '<my-thoughts>\\n- The code uses gsub to replace \".\" with \"a\".\\n- The code does not assign the result back to the vector, but this is acceptable.\\n- The code is written in two lines, which is acceptable.\\n</my-thoughts>\\n<explanation>No points are deducted as the code correctly replaces \".\" with \"a\" and follows the rubric guidelines. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R15': \"<my-thoughts>\\n- The student attempted to change the datatypes using `as.numeric(levels(horrible_numbers))`.\\n- The code does not produce the correct mean because `levels(horrible_numbers)` returns the unique levels of the factor, not the actual values.\\n</my-thoughts>\\n<explanation>Subtracting 0.5 points because the student's code does not produce the correct mean. Calculation: 1 - 0.5 = 0.5</explanation><points>0.5</points>\",\n",
       " '#R16': '<my-thoughts>\\n- The code produces the correct result for the length of the third side c.\\n- Variable definitions for a, b, and delta are correct.\\n- The conversion of the angle to radians is correct.\\n- The application of the Law of Cosines formula is correct.\\n- The result is stored in variable c and printed.\\n</my-thoughts>\\n<explanation>No points deducted as all variable definitions are correct and the applied formula is correct. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R17': '<my-thoughts>\\n- The code generates a sequence of odd numbers from 1 to 100.\\n- This sequence contains the first 50 odd positive integers.\\n- The product of the generated sequence is computed correctly using the `prod` function.\\n</my-thoughts>\\n<explanation>No points are deducted as the generated vector contains the first 50 odd integers and the product is computed correctly. Calculation: 1 - 0 - 0 = 1</explanation><points>1</points>',\n",
       " '#R18': '<my-thoughts>\\n- The code sets a seed, ensuring reproducibility.\\n- The number of rows and columns are chosen randomly.\\n- The matrix is initialized without specifying NA values, but it is empty which is acceptable.\\n- The dimensions of the matrix are shown using the dim() function.\\n</my-thoughts>\\n<explanation>No points are deducted as the code meets all the requirements: seed is set, rows and columns are random, matrix is initialized (empty is acceptable), and dimensions are shown. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R19': '<my-thoughts>\\n- The code correctly identifies EEGData3 as incorrect.\\n- The student provides an explanation about the order of arguments in functions.\\n- The explanation is clear and provides insight into function arguments.\\n</my-thoughts>\\n<explanation>No points deducted as the student correctly identified the incorrect matrix and provided insights about function arguments. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R20': '<my-thoughts>\\n- The code produces the correct counts per participant.\\n- The student used colSums(M) which correctly computes the number of correct responses per participant.\\n- The code matches the rubric requirements.\\n</my-thoughts>\\n<explanation>No points are deducted because the student correctly computes the counts per participant. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R21': '<my-thoughts>\\n- The code produces a 2x2x2 array, which matches the rubric.\\n- The student specifies dimension names, which matches the rubric.\\n- The dimension names differ from the example solution, but this is allowed by the rubric.\\n</my-thoughts>\\n<explanation>No points are deducted as the student created a 2x2x2 array and specified dimension names. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R22': '<my-thoughts>\\n- The student correctly uses position-based indexing to fill in the relationship between Ron and Hermione.\\n- The student correctly uses name-based indexing to fill in the relationship between Snape and Harry.\\n- The student does not replace any NAs in the initial matrix definition.\\n</my-thoughts>\\n<explanation>No points are deducted as the student followed all the instructions correctly. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R23': '<my-thoughts>\\n- The student added only one line of code.\\n- The diagonal elements are successfully changed to NA.\\n- The code is correct and matches the example solution.\\n</my-thoughts>\\n<explanation>No points are deducted as the student followed the instructions correctly and the diagonal elements are successfully changed to NA. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R24': '<my-thoughts>\\n- The code produces the correct vector: \"a\", \"a\", \"b\", \"b\", \"c\", \"c\", \"d\", \"d\".\\n- The rep function is used correctly.\\n- The solution works as intended.\\n</my-thoughts>\\n<explanation>No points are deducted as the solution works and uses the rep function correctly. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R25': '<my-thoughts>\\n- The code produces the correct dataframe.\\n- The student used shortcuts for \"First_appearance\", \"Morality\", and \"Dating_appeal\".\\n- The dataframe is correctly created and matches the example solution.\\n</my-thoughts>\\n<explanation>No points deducted as the student used shortcuts and created the correct dataframe. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R26': '<my-thoughts>\\n- The code produces the correct final_grade column.\\n- The column name is correct.\\n- The student added only one additional line of code.\\n- The new column is computed correctly.\\n</my-thoughts>\\n<explanation>No points deducted as the student followed all the instructions correctly. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R27': \"<my-thoughts>\\n- The code correctly creates a boolean variable 'passed' in the grades dataframe.\\n- The logical operation is done correctly, checking both final_grade and exam.\\n- The task also asks to print the IDs of the passing students, which is not done in the student's answer.\\n</my-thoughts>\\n<explanation>Subtracting 1 point because the student did not print the IDs of the passing students as required by the task. Calculation: 1 - 1 = 0</explanation><points>0</points>\",\n",
       " '#R28': '<my-thoughts>\\n- The code uses the write.csv function correctly.\\n- The file name is different but that is acceptable.\\n- The row.names parameter is set to FALSE, which matches the requirement to remove row names.\\n</my-thoughts>\\n<explanation>The student correctly used the write.csv function and set row.names to FALSE to ensure that row names are not included in the file. The file name difference is acceptable and does not affect the correctness of the solution. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R29': '<my-thoughts>\\n- The code installs the required packages, which is not necessary for the task but does not affect the grading.\\n- The code correctly gets a quote from the statquotes package.\\n- The code correctly uses the cowsay package to have an animal say the quote.\\n- The animal is randomly selected using the sample function.\\n</my-thoughts>\\n<explanation>No points are deducted as the quote > say pipeline is implemented and the animal is randomly selected. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#R30': '<my-thoughts>\\n- The code reads the dataset from the URL correctly.\\n- The data indexing/filtering is done correctly to create the vectors.\\n- The t-test is included and correctly implemented.\\n</my-thoughts>\\n<explanation>No points are deducted as all parts of the task are correctly implemented. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#Radv1': '<my-thoughts>\\n- The first vector creation using `c(1,2,3)` is correct.\\n- The second vector creation using `1:3` is correct.\\n- The third vector creation using `seq(1:3)` is incorrect; it should be `seq(1, 3)`.\\n- The fourth vector creation using `rep(c(1,2,3), each=1)` is correct.\\n- The datatype check using `typeof(vec)` is correct.\\n</my-thoughts>\\n<explanation>Subtracting 0.25 points because the third vector creation is incorrect. Calculation: 1 - 0.25 = 0.75</explanation><points>0.75</points>',\n",
       " '#Radv2': '<my-thoughts>\\n- The code uses the built-in dataset `cars`, which matches the rubric.\\n- The student successfully indexed a row with `cars[1, ]`.\\n- The student successfully indexed a column with `cars$speed`.\\n- The student successfully indexed a specific cell with `cars$speed[1]`.\\n</my-thoughts>\\n<explanation>No points are deducted as the student successfully used a built-in dataset and demonstrated indexing a row, a column, and a specific cell. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#Radv3': '<my-thoughts>\\n- The code installs the package using install.packages(), which is correct.\\n- The code does not use the library() function, which is correct.\\n- The code uses the :: operator to call a function from the package, which is correct.\\n- The code uses a function from the stringr package, which is correct.\\n</my-thoughts>\\n<explanation>No points are deducted as the student followed all the requirements correctly. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#Radv4': '<my-thoughts>\\n- The code produces a binary variable \"animal\" with values \"cat\" and \"dog\", which matches the rubric.\\n- The code produces a continuous variable \"cuteness\", which matches the rubric.\\n- The simulation of the continuous variable \"cuteness\" implies group differences, as the mean for \"cat\" is 4 and for \"dog\" is 10, which matches the rubric.\\n- The variables are not put into a dataframe together, which is a requirement of the task.\\n</my-thoughts>\\n<explanation>Subtracting 0.5 points because the variables are not put into a dataframe together. Calculation: 1 - 0.5 = 0.5</explanation><points>0.5</points>',\n",
       " '#Radv5': '<my-thoughts>\\n- The code creates a vector with missing values and a data frame.\\n- The is.na() function is used for imputation.\\n- Missing values are imputed with the mean.\\n- No comments are provided.\\n</my-thoughts>\\n<explanation>Subtracting 0.5 points because no comments are provided. Calculation: 1 - 0.5 = 0.5</explanation><points>0.5</points>',\n",
       " '#Radv6': '<my-thoughts>\\n- The code produces a list with named elements, which matches the rubric.\\n- The reason for using a list is provided, but it is not concise and could be more compelling.\\n</my-thoughts>\\n<explanation>Subtracting 0.5 points because the reason for using a list is not compelling. Calculation: 1 - 0.5 = 0.5</explanation><points>0.5</points>',\n",
       " '#Radv7': '<my-thoughts>\\n- The student provided a link to a discussion on stackoverflow.\\n- The student gave a reason for finding the discussion interesting.\\n- The reason given is coherent and relevant to the topic.\\n</my-thoughts>\\n<explanation>No points are deducted as the student provided both a link and a coherent reason. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#Radv8': '<my-thoughts>\\n- The code reads a dataset from a URL, which matches the rubric.\\n- The code creates a plot, which matches the rubric.\\n</my-thoughts>\\n<explanation>No points are deducted as the student has successfully read a dataset from a URL and created a plot. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#Python1': '<my-thoughts>\\n- The student provided the correct Python code.\\n- The student mentioned `as.numeric()` instead of `as.integer()`.\\n- This matches the rubric for a 0.25 point deduction.\\n</my-thoughts>\\n<explanation>Subtracting 0.25 points because the student used `as.numeric` instead of `as.integer`. Calculation: 1 - 0.25 = 0.75</explanation><points>0.75</points>',\n",
       " '#Python2': '<my-thoughts>\\n- The student provided bash commands to install numpy and pandas.\\n- The student used two separate pip install commands.\\n- Both numpy and pandas are being installed.\\n</my-thoughts>\\n<explanation>No points are deducted because both numpy and pandas are installed. Calculation: 1 - 0 - 0 = 1</explanation><points>1</points>',\n",
       " '#Python3': '<my-thoughts>\\n- The student added the line `import numpy as np` above the code.\\n- This matches the rubric requirement.\\n</my-thoughts>\\n<explanation>No points are deducted as the student correctly added `import numpy as np` above the code. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#Python4': '<my-thoughts>\\n- The student correctly identified the need to copy the array.\\n- The student changed only one line of code.\\n- The explanation provided by the student is accurate and relevant.\\n</my-thoughts>\\n<explanation>No points are deducted as the student made the correct change in one line and provided a proper explanation. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#Python5': '<my-thoughts>\\n- The code produces the correct mean by using np.nanmean(), which correctly handles the NaN value.\\n- The student did not manually remove np.nan from the first line of code.\\n- The code computes the correct mean.\\n</my-thoughts>\\n<explanation>No points are deducted as the student did not manually remove np.nan and the code computes the correct mean. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#Python6': '<my-thoughts>\\n- The code produces a 3-dimensional array.\\n- The dimensions of the array are 4x3x5.\\n- All elements in the array are zero.\\n- The use of square brackets instead of parentheses is acceptable in this context.\\n</my-thoughts>\\n<explanation>No points are deducted as the array has the correct dimensions and all elements are zero. Calculation: 1 - 0 = 1</explanation><points>1</points>',\n",
       " '#Python7': '<my-thoughts>\\n- The code uses np.arange(1, 100, step=2) which correctly generates the first 50 odd integers.\\n- The code uses np.prod() to calculate the product of the array elements.\\n- The code does not include an import statement for numpy.\\n</my-thoughts>\\n<explanation>Subtracting 0.5 points because the code does not include an import statement for numpy, which is necessary for the code to run correctly. Calculation: 1 - 0.5 = 0.5</explanation><points>0.5</points>',\n",
       " '#Python8': '<my-thoughts>\\n- The code produces the correct dataframe.\\n- The student used shortcuts for all columns except the \"Name\" column.\\n- There are no typos in variable names or strings.\\n</my-thoughts>\\n<explanation>Subtracting 0.5 points because the \"Name\" column is fully typed out. Calculation: 1 - 0.5 = 0.5</explanation><points>0.5</points>',\n",
       " '#Python9': '<my-thoughts>\\n- The code imports the os module and retrieves the filenames in the current directory using os.listdir().\\n- However, the filenames are not combined into one string.\\n</my-thoughts>\\n<explanation>Subtracting 0.5 points because the filenames are not combined into one string. Calculation: 1 - 0.5 = 0.5</explanation><points>0.5</points>',\n",
       " '#Python10': '<my-thoughts>\\n- The code produces a dictionary called my_measurements, which matches the rubric.\\n- The first key is called my_mood_measurements, which matches the rubric.\\n- The first key contains 365 binary values (\"happy\" or \"sad\"), which matches the rubric.\\n- The second key is called my_iq_measurements, which matches the rubric.\\n- The second key contains 52 normally distributed values, which matches the rubric.\\n- The standard deviation for the normally distributed values is 15 instead of 3.\\n</my-thoughts>\\n<explanation>Subtracting 0.5 points because the values of the second key are not 52 normally distributed values with the correct standard deviation. Calculation: 1 - 0.5 = 0.5</explanation><points>0.5</points>'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grading_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Grading report\n",
    "pdf = MarkdownPdf()\n",
    "\n",
    "# Header, summary, preliminary grade, coding challenge\n",
    "section = f\"# LLM Grading Assignment {ASSIGNMENT_ID}\\n\\n\"\n",
    "section += f\"**Points R:** {points_r}\\n\"\n",
    "section += f\"**Points Radv:** {points_radv}\\n\"\n",
    "section += f\"**Points Python:** {points_py}\\n\"\n",
    "section += f\"{used_adv}\\n\"\n",
    "section += f\"**Preliminary grade:** {grade}/10\\n\\n\"\n",
    "\n",
    "\n",
    "# Questionwise grading\n",
    "for key, value in grading_dict.items():\n",
    "    # section = \"\"\n",
    "    section += f\"## {key} ({extract_html_content(value, \"points\")}/1)\\n\"\n",
    "    section += extract_html_content(value, \"explanation\")\n",
    "    section += \"\\n\\n\"\n",
    "pdf.add_section(Section(section))\n",
    "\n",
    "# Save pdf\n",
    "student_grading_report_out_path = f\"{SUBMISSIONS_PATH}/user-{user_id}/assignment-{ASSIGNMENT_ID}/llm_outputs/user-{user_id}_ass-{ASSIGNMENT_ID}_try-{attempt}_LLMGrading.pdf\"\n",
    "pdf.save(student_grading_report_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This week was all about enhancing your skills in R, especially with regard to using built-in functions, handling data types, managing directories, and applying statistical functions. Overall, you are progressing well, especially in understanding variables and built-in packages. However, there are areas where you can improve, such as sorting data before indexing and managing factors and NA values. Make sure to work on adding more detailed comments to clarify your code and avoid minor mistakes.\\n\\nI recommend reviewing the feedback, particularly for questions R2, R11, R15, and R27 to refine your skills. To help address your current areas of improvement, I've created a personalized coding challenge for you.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_html_content(completion.choices[0].message.content, \"summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload LLM Grading and feedback to Canvas\n",
    "For all students who were graded for this assignment, the LLM generated grade is uploaded together with some predetermined comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Canvas objects \n",
    "course = canvas_client.get_course(COURSE_ID)\n",
    "assignment = course.get_assignment(ASSIGNMENT_ID)\n",
    "quiz = course.get_quiz(QUIZ_ID)\n",
    "quiz_submissions = [quiz_submission for quiz_submission in quiz.get_submissions()]\n",
    "\n",
    "# Load text for comments to canvas\n",
    "comment_preliminary_grade = open(f\"{RESOURCES_PATH}/canvas_comments/canvas-comment_preliminary_grade.txt\", \"r\").read()\n",
    "comment_feedback_received = open(f\"{RESOURCES_PATH}/canvas_comments/canvas-comment_feedback_received.txt\", \"r\").read()\n",
    "\n",
    "# Get grading files\n",
    "grading_files = create_file_list(SUBMISSIONS_PATH, [f\"ass-{ASSIGNMENT_ID}\", f\"grader-{MODEL}_que-combined.csv\"],[\".json\"])\n",
    "grading_files = deduplicate_highest_attempt(grading_files)\n",
    "\n",
    "for f in grading_files:\n",
    "    \n",
    "    user_id = int(re.compile(r\"user-(\\d+)\").search(f).group(1))\n",
    "    if datetime.today() >= datetime.strptime(LOCK_GRADES_DATE, \"%Y-%m-%d\") and str(user_id) != \"513294\":\n",
    "        print(\"WARNING GRADES ARE LOCKED AND NO UPDATES TO CANVAS ARE MADE!\")\n",
    "        continue\n",
    "\n",
    "    file_list_indidividual_questions = create_file_list(SUBMISSIONS_PATH, \n",
    "                                                        [f\"ass-{ASSIGNMENT_ID}\", f\"grader-{MODEL}\", f\"user-{user_id}\"],\n",
    "                                                        [\".json\", \"que-combined\"])\n",
    "    explanations = []\n",
    "    for file in file_list_indidividual_questions:\n",
    "        df = pd.read_csv(file)\n",
    "        explanations.append(f\"{df.question.values[0]}\\n{df.explanation.values[0]}\")\n",
    "    comment_explanation = \"Explanations for grading:\\n\\n\" + \"\\n\\n\".join(explanations)\n",
    "\n",
    "    dat = pd.read_csv(f)\n",
    "\n",
    "    canvas_submission = assignment.get_submission(user = user_id)\n",
    "\n",
    "    # Update Canvas grade\n",
    "    update_canvas_grade(user_id,\n",
    "                        R_QUIZ_QUESTION_ID,\n",
    "                        ADV_QUIZ_QUESTION_ID,\n",
    "                        quiz_submissions,\n",
    "                        dat.points_r.values[0],\n",
    "                        dat.points_adv.values[0],\n",
    "                        dat.used_adv.values[0],\n",
    "                        dat.grade.values[0],\n",
    "                        canvas_submission)\n",
    "\n",
    "    # Post comments with grade and feedback\n",
    "    post_canvas_comments(canvas_submission, comments=[comment_preliminary_grade, \n",
    "                                                      comment_explanation,\n",
    "                                                      comment_feedback_received])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pips-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
